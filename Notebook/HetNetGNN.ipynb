{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9HZHDjK9CAYl",
    "outputId": "88b1b9d6-6cf9-40ac-a2c7-eae1bedcc726"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'GNN-Resource-Management'...\n",
      "remote: Enumerating objects: 209, done.\u001b[K\n",
      "remote: Counting objects: 100% (209/209), done.\u001b[K\n",
      "remote: Compressing objects: 100% (141/141), done.\u001b[K\n",
      "remote: Total 209 (delta 110), reused 158 (delta 62), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (209/209), 236.41 KiB | 1.27 MiB/s, done.\n",
      "Resolving deltas: 100% (110/110), done.\n",
      "/content/GNN-Resource-Management/NewDir\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/LeGiangK62/GNN-Resource-Management.git\n",
    "%cd GNN-Resource-Management/NewDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "euhBKmR3CSf0"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import torch\n",
    "!pip install torch_geometric\n",
    "\n",
    "# Optional dependencies:\n",
    "if torch.cuda.is_available():\n",
    "  !pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.0+cu118.html\n",
    "else:\n",
    "  !pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.0.0+cpu.html\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PSrar76fCI8E"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from torch.nn import Sequential as Seq, Linear as Lin, ReLU, Sigmoid\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import Linear, HGTConv\n",
    "\n",
    "from WSN_GNN import generate_channels_wsn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9cSBfHOVCgbE"
   },
   "source": [
    "# Create HeteroData from the wireless system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "id": "Lqs7nC0tCOYM"
   },
   "outputs": [],
   "source": [
    "#region Create HeteroData from the wireless system\n",
    "def convert_to_hetero_data(channel_matrices):\n",
    "    graph_list = []\n",
    "    num_sam, num_aps, num_users = channel_matrices.shape\n",
    "    for i in range(num_sam):\n",
    "        x1 = torch.ones(num_users, 1)\n",
    "        x2 = torch.ones(num_users, 1)  # power allocation\n",
    "        x3 = torch.ones(num_users, 1)  # ap selection?\n",
    "        user_feat = torch.cat((x1,x2,x3),1)  # features of user_node\n",
    "        ap_feat = torch.zeros(num_aps, num_aps_features)  # features of user_node\n",
    "        edge_feat_uplink = channel_matrices[i, :, :].reshape(1, -1)\n",
    "        edge_feat_downlink = channel_matrices[i, :, :].reshape(1, -1)\n",
    "        graph = HeteroData({\n",
    "            'user': {'x': user_feat},\n",
    "            'ap': {'x': ap_feat}\n",
    "        })\n",
    "        # Create edge types and building the graph connectivity:\n",
    "        graph['user', 'uplink', 'ap'].edge_attr = torch.tensor(edge_feat_uplink, dtype=torch.float)\n",
    "        graph['ap', 'downlink', 'user'].edge_attr = torch.tensor(edge_feat_downlink, dtype=torch.float)\n",
    "        graph['user', 'uplink', 'ap'].edge_index = torch.tensor(adj_matrix(num_users, num_aps).transpose(), \n",
    "                                                                dtype=torch.int64)\n",
    "        graph['ap', 'downlink', 'user'].edge_index = torch.tensor(adj_matrix(num_aps, num_users).transpose(),\n",
    "                                                                dtype=torch.int64)\n",
    "\n",
    "        # graph['ap', 'downlink', 'user'].edge_attr  = torch.tensor(edge_feat_downlink, dtype=torch.float)\n",
    "        graph_list.append(graph)\n",
    "    return graph_list\n",
    "\n",
    "\n",
    "def adj_matrix(num_from, num_dest):\n",
    "    adj = []\n",
    "    for i in range(num_from):\n",
    "        for j in range(num_dest):\n",
    "            adj.append([i, j])\n",
    "    return np.array(adj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMJBmS42Ckc-"
   },
   "source": [
    "# Build Heterogeneous GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "id": "3Oigpa4mCVBk"
   },
   "outputs": [],
   "source": [
    "#region Build Heterogeneous GNN\n",
    "class HetNetGNN(torch.nn.Module):\n",
    "    def __init__(self, data, hidden_channels, out_channels, num_heads, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lin_dict = torch.nn.ModuleDict()\n",
    "        for node_type in data.node_types:\n",
    "            self.lin_dict[node_type] = Linear(-1, hidden_channels)\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            conv = HGTConv(hidden_channels, hidden_channels, data.metadata(),\n",
    "                           num_heads, group='sum')\n",
    "            self.convs.append(conv)\n",
    "\n",
    "        self.lin = Linear(hidden_channels, out_channels)\n",
    "\n",
    "        self.lin1 = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        original = x_dict['user'].clone\n",
    "        x_dict = {\n",
    "            node_type: self.lin_dict[node_type](x).relu_()\n",
    "            for node_type, x in x_dict.items()\n",
    "        }\n",
    "\n",
    "        for conv in self.convs:\n",
    "            x_dict = conv(x_dict, edge_index_dict)\n",
    "\n",
    "        original = x_dict['user'] # not original\n",
    "        power = self.lin(x_dict['user'])\n",
    "        ap_selection = self.lin1(x_dict['user'])\n",
    "        ap_selection = torch.abs(ap_selection).int()\n",
    "        \n",
    "        out = torch.cat((original[:,1].unsqueeze(-1), power[:,1].unsqueeze(-1), ap_selection[:,1].unsqueeze(-1)), 1)\n",
    "        return out\n",
    "\n",
    "class EdgeConv(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def foward(self, graph, inputs):\n",
    "        return 1\n",
    "\n",
    "\n",
    "class RoundActivation(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.round(torch.abs(x))\n",
    "\n",
    "#endregion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import MessagePassing\n",
    "import torch_geometric.utils as pyg_utils\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def MLP(channels, batch_norm=True):\n",
    "    layers = []\n",
    "    for i in range(1, len(channels)):\n",
    "        layers.append(nn.Linear(channels[i - 1], channels[i]))\n",
    "        layers.append(nn.ReLU())\n",
    "        if batch_norm:\n",
    "            layers.append(nn.BatchNorm1d(channels[i]))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class EdgeConv(MessagePassing):\n",
    "    def __init__(self, input_dim, node_dim, aggr='mean', **kwargs):\n",
    "        super(EdgeConv, self).__init__(aggr=aggr)\n",
    "        self.lin = MLP([input_dim, 32])\n",
    "        self.res_lin = nn.Linear(node_dim, 32)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x = self.lin(edge_attr)  # Process edge attributes with MLP\n",
    "        return self.propagate(edge_index, x=x)\n",
    "\n",
    "    def message(self, x_j):\n",
    "        return x_j\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        return aggr_out + self.res_lin(x)\n",
    "\n",
    "class RGCN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = EdgeConv(input_dim=4, node_dim=1)\n",
    "        self.conv2 = EdgeConv(input_dim=66, node_dim=32)\n",
    "        self.conv3 = EdgeConv(input_dim=66, node_dim=32)\n",
    "        self.mlp = MLP([32, 16])\n",
    "        self.mlp = nn.Sequential(self.mlp, nn.Linear(16, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = self.conv3(x, edge_index, edge_attr)\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Note: The input data format in `torch_geometric` might differ from what is used in the original code.\n",
    "# Make sure to preprocess your input data accordingly to create `data` objects for the RGCN model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xARv-2yLCnwN"
   },
   "source": [
    "# Training and Testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "id": "3zupX9hyCXoc"
   },
   "outputs": [],
   "source": [
    "#region Training and Testing functions\n",
    "def loss_function(output, batch, is_train=True):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    num_user = batch['user']['x'].shape[0]\n",
    "    num_ap = batch['ap']['x'].shape[0]\n",
    "    ##\n",
    "    channel_matrix = batch['user', 'ap']['edge_attr']\n",
    "    ##\n",
    "#     power_max = batch['user']['x'][:, 0]\n",
    "#     power = batch['ue']['x'][:, 1]\n",
    "#     ap_selection = batch['ue']['x'][:, 2]\n",
    "    power_max = output[:, 0]\n",
    "    power = output[:, 1]\n",
    "    ap_selection = output[:, 2]\n",
    "    ##\n",
    "    ap_selection = ap_selection.int()\n",
    "    index = torch.arange(num_user)\n",
    "\n",
    "    G = torch.reshape(channel_matrix, (-1, num_ap, num_user))\n",
    "    # P = torch.reshape(power, (-1, num_ap, num_user)) #* p_max\n",
    "    P = torch.zeros_like(G, requires_grad=True).clone()\n",
    "    P[0, ap_selection[index], index] = power_max * power\n",
    "    ##\n",
    "    \n",
    "    # new_noise = torch.from_numpy(noise_matrix).to(device)\n",
    "    desired_signal = torch.sum(torch.mul(P, G), dim=1).unsqueeze(-1)\n",
    "    G_UE = torch.sum(G, dim=2).unsqueeze(-1)\n",
    "    all_signal = torch.matmul(P.permute((0,2,1)), G_UE)\n",
    "    interference = all_signal - desired_signal #+ new_noise\n",
    "    rate = torch.log(1 + torch.div(desired_signal, interference))\n",
    "    sum_rate = torch.mean(torch.sum(rate, 1))\n",
    "    mean_power = torch.mean(torch.sum(P.permute((0,2,1)), 1))\n",
    "\n",
    "    if is_train:\n",
    "        return torch.neg(sum_rate / mean_power)\n",
    "    else:\n",
    "        return sum_rate / mean_power\n",
    "\n",
    "\n",
    "\n",
    "def train(data_loader):\n",
    "    model.train()\n",
    "    device_type = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    total_examples = total_loss = 0\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        batch = batch.to(device_type)\n",
    "        # batch_size = batch['ue'].batch_size\n",
    "        out = model(batch.x_dict, batch.edge_index_dict)\n",
    "        tmp_loss = loss_function(out, batch, True)\n",
    "        tmp_loss.backward()\n",
    "        optimizer.step()\n",
    "        #total_examples += batch_size\n",
    "        total_loss += float(tmp_loss) #* batch_size\n",
    "\n",
    "    return total_loss #/ total_examples\n",
    "\n",
    "\n",
    "def test(data_loader):\n",
    "    model.eval()\n",
    "    device_type = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    total_examples = total_loss = 0\n",
    "    for batch in data_loader:\n",
    "        batch = batch.to(device_type)\n",
    "        # batch_size = batch['ue'].batch_size\n",
    "        out = model(batch.x_dict, batch.edge_index_dict)\n",
    "        tmp_loss = loss_function(out, batch, False)\n",
    "        #total_examples += batch_size\n",
    "        total_loss += float(tmp_loss) #* batch_size\n",
    "\n",
    "    return total_loss #/ total_examples\n",
    "#endregion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYnfbYmsCa59"
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7acs4Y0uCdoD"
   },
   "outputs": [],
   "source": [
    "K = 3  # number of APs\n",
    "N = 5  # number of nodes\n",
    "R = 10  # radius\n",
    "\n",
    "num_users_features = 3\n",
    "num_aps_features = 3\n",
    "\n",
    "num_train = 2  # number of training samples\n",
    "num_test = 4  # number of test samples\n",
    "\n",
    "reg = 1e-2\n",
    "pmax = 1\n",
    "var_db = 10\n",
    "var = 1 / 10 ** (var_db / 10)\n",
    "var_noise = 10e-11\n",
    "\n",
    "power_threshold = 2.0\n",
    "\n",
    "X_train, noise_train, pos_train, adj_train, index_train = generate_channels_wsn(K, N, num_train, var_noise, R)\n",
    "X_test, noise_test, pos_test, adj_test, index_test = generate_channels_wsn(K + 1, N + 10, num_test, var_noise, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "hCH-zKEADSQ8"
   },
   "outputs": [],
   "source": [
    "# Maybe need normalization here\n",
    "train_data = convert_to_hetero_data(X_train)\n",
    "test_data = convert_to_hetero_data(X_test)\n",
    "\n",
    "batchSize = 1\n",
    "\n",
    "train_loader = DataLoader(train_data, batchSize, shuffle=True, num_workers=1)\n",
    "test_loader = DataLoader(test_data, batchSize, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "id": "0_RqWpsQDVmV"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "data = train_data[0]\n",
    "data = data.to(device)\n",
    "\n",
    "model = HetNetGNN(data, hidden_channels=64, out_channels=4, num_heads=2, num_layers=1)\n",
    "model = model.to(device)\n",
    "\n",
    "# # print(data.edge_index_dict)\n",
    "# with torch.no_grad():\n",
    "#     output = model(data.x_dict, data.edge_index_dict)\n",
    "# print(output)\n",
    "# print(data)\n",
    "\n",
    "# data = test_data[0]\n",
    "# data = data.to(device)\n",
    "#\n",
    "# with torch.no_grad():\n",
    "#     output = model(data.x_dict, data.edge_index_dict)\n",
    "#     print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJYiXcE9Cs2M"
   },
   "source": [
    "## Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "_AdzgAL4CqOM",
    "outputId": "e588b0d7-1a7e-4916-c594-ed5fd14d4add"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Loss: 47.0816, Test Reward: -29.8976\n",
      "Epoch: 002, Train Loss: 29.0778, Test Reward: -20.5468\n",
      "Epoch: 003, Train Loss: 20.1887, Test Reward: -15.2608\n",
      "Epoch: 004, Train Loss: 15.2918, Test Reward: -11.8585\n",
      "Epoch: 005, Train Loss: 11.9738, Test Reward: -9.5137\n",
      "Epoch: 006, Train Loss: 9.6070, Test Reward: -7.8197\n",
      "Epoch: 007, Train Loss: 7.9350, Test Reward: -6.5465\n",
      "Epoch: 008, Train Loss: 6.7002, Test Reward: -5.5615\n",
      "Epoch: 009, Train Loss: 5.7096, Test Reward: -4.7869\n",
      "Epoch: 010, Train Loss: 4.9084, Test Reward: -4.1693\n",
      "Epoch: 011, Train Loss: 4.3020, Test Reward: -3.6669\n",
      "Epoch: 012, Train Loss: 3.7791, Test Reward: -3.2559\n",
      "Epoch: 013, Train Loss: 3.3627, Test Reward: -2.9148\n",
      "Epoch: 014, Train Loss: 3.0164, Test Reward: -2.6290\n",
      "Epoch: 015, Train Loss: 2.7255, Test Reward: -2.3874\n",
      "Epoch: 016, Train Loss: 2.4790, Test Reward: -2.1814\n",
      "Epoch: 017, Train Loss: 2.2738, Test Reward: -2.0038\n",
      "Epoch: 018, Train Loss: 2.0867, Test Reward: -1.8507\n",
      "Epoch: 019, Train Loss: 1.9296, Test Reward: -1.7172\n",
      "Epoch: 020, Train Loss: 1.7959, Test Reward: -1.5996\n",
      "Epoch: 021, Train Loss: 1.6714, Test Reward: -1.4962\n",
      "Epoch: 022, Train Loss: 1.5648, Test Reward: -1.4043\n",
      "Epoch: 023, Train Loss: 1.4699, Test Reward: -1.3222\n",
      "Epoch: 024, Train Loss: 1.3872, Test Reward: -1.2482\n",
      "Epoch: 025, Train Loss: 1.3104, Test Reward: -1.1814\n",
      "Epoch: 026, Train Loss: 1.2392, Test Reward: -1.1211\n",
      "Epoch: 027, Train Loss: 1.1765, Test Reward: -1.0661\n",
      "Epoch: 028, Train Loss: 1.1194, Test Reward: -1.0157\n",
      "Epoch: 029, Train Loss: 1.0684, Test Reward: -0.9693\n",
      "Epoch: 030, Train Loss: 1.0187, Test Reward: -0.9267\n",
      "Epoch: 031, Train Loss: 0.9744, Test Reward: -0.8873\n",
      "Epoch: 032, Train Loss: 0.9343, Test Reward: -0.8506\n",
      "Epoch: 033, Train Loss: 0.8949, Test Reward: -0.8165\n",
      "Epoch: 034, Train Loss: 0.8594, Test Reward: -0.7848\n",
      "Epoch: 035, Train Loss: 0.8271, Test Reward: -0.7549\n",
      "Epoch: 036, Train Loss: 0.7958, Test Reward: -0.7270\n",
      "Epoch: 037, Train Loss: 0.7665, Test Reward: -0.7007\n",
      "Epoch: 038, Train Loss: 0.7382, Test Reward: -0.6761\n",
      "Epoch: 039, Train Loss: 0.7125, Test Reward: -0.6529\n",
      "Epoch: 040, Train Loss: 0.6888, Test Reward: -0.6308\n",
      "Epoch: 041, Train Loss: 0.6650, Test Reward: -0.6101\n",
      "Epoch: 042, Train Loss: 0.6433, Test Reward: -0.5904\n",
      "Epoch: 043, Train Loss: 0.6226, Test Reward: -0.5717\n",
      "Epoch: 044, Train Loss: 0.6030, Test Reward: -0.5540\n",
      "Epoch: 045, Train Loss: 0.5844, Test Reward: -0.5371\n",
      "Epoch: 046, Train Loss: 0.5671, Test Reward: -0.5209\n",
      "Epoch: 047, Train Loss: 0.5501, Test Reward: -0.5056\n",
      "Epoch: 048, Train Loss: 0.5336, Test Reward: -0.4910\n",
      "Epoch: 049, Train Loss: 0.5186, Test Reward: -0.4769\n",
      "Epoch: 050, Train Loss: 0.5039, Test Reward: -0.4636\n",
      "Epoch: 051, Train Loss: 0.4894, Test Reward: -0.4508\n",
      "Epoch: 052, Train Loss: 0.4764, Test Reward: -0.4386\n",
      "Epoch: 053, Train Loss: 0.4631, Test Reward: -0.4269\n",
      "Epoch: 054, Train Loss: 0.4512, Test Reward: -0.4156\n",
      "Epoch: 055, Train Loss: 0.4390, Test Reward: -0.4049\n",
      "Epoch: 056, Train Loss: 0.4277, Test Reward: -0.3946\n",
      "Epoch: 057, Train Loss: 0.4169, Test Reward: -0.3847\n",
      "Epoch: 058, Train Loss: 0.4065, Test Reward: -0.3752\n",
      "Epoch: 059, Train Loss: 0.3964, Test Reward: -0.3660\n",
      "Epoch: 060, Train Loss: 0.3868, Test Reward: -0.3572\n",
      "Epoch: 061, Train Loss: 0.3775, Test Reward: -0.3487\n",
      "Epoch: 062, Train Loss: 0.3688, Test Reward: -0.3405\n",
      "Epoch: 063, Train Loss: 0.3600, Test Reward: -0.3327\n",
      "Epoch: 064, Train Loss: 0.3517, Test Reward: -0.3251\n",
      "Epoch: 065, Train Loss: 0.3439, Test Reward: -0.3178\n",
      "Epoch: 066, Train Loss: 0.3362, Test Reward: -0.3107\n",
      "Epoch: 067, Train Loss: 0.3285, Test Reward: -0.3039\n",
      "Epoch: 068, Train Loss: 0.3213, Test Reward: -0.2973\n",
      "Epoch: 069, Train Loss: 0.3144, Test Reward: -0.2910\n",
      "Epoch: 070, Train Loss: 0.3077, Test Reward: -0.2849\n",
      "Epoch: 071, Train Loss: 0.3013, Test Reward: -0.2789\n",
      "Epoch: 072, Train Loss: 0.2952, Test Reward: -0.2732\n",
      "Epoch: 073, Train Loss: 0.2890, Test Reward: -0.2676\n",
      "Epoch: 074, Train Loss: 0.2833, Test Reward: -0.2622\n",
      "Epoch: 075, Train Loss: 0.2776, Test Reward: -0.2570\n",
      "Epoch: 076, Train Loss: 0.2719, Test Reward: -0.2520\n",
      "Epoch: 077, Train Loss: 0.2668, Test Reward: -0.2471\n",
      "Epoch: 078, Train Loss: 0.2615, Test Reward: -0.2424\n",
      "Epoch: 079, Train Loss: 0.2566, Test Reward: -0.2378\n",
      "Epoch: 080, Train Loss: 0.2518, Test Reward: -0.2333\n",
      "Epoch: 081, Train Loss: 0.2471, Test Reward: -0.2290\n",
      "Epoch: 082, Train Loss: 0.2424, Test Reward: -0.2249\n",
      "Epoch: 083, Train Loss: 0.2381, Test Reward: -0.2208\n",
      "Epoch: 084, Train Loss: 0.2338, Test Reward: -0.2168\n",
      "Epoch: 085, Train Loss: 0.2295, Test Reward: -0.2130\n",
      "Epoch: 086, Train Loss: 0.2256, Test Reward: -0.2093\n",
      "Epoch: 087, Train Loss: 0.2216, Test Reward: -0.2057\n",
      "Epoch: 088, Train Loss: 0.2179, Test Reward: -0.2022\n",
      "Epoch: 089, Train Loss: 0.2142, Test Reward: -0.1988\n",
      "Epoch: 090, Train Loss: 0.2105, Test Reward: -0.1954\n",
      "Epoch: 091, Train Loss: 0.2070, Test Reward: -0.1922\n",
      "Epoch: 092, Train Loss: 0.2036, Test Reward: -0.1890\n",
      "Epoch: 093, Train Loss: 0.2003, Test Reward: -0.1860\n",
      "Epoch: 094, Train Loss: 0.1970, Test Reward: -0.1830\n",
      "Epoch: 095, Train Loss: 0.1939, Test Reward: -0.1801\n",
      "Epoch: 096, Train Loss: 0.1907, Test Reward: -0.1773\n",
      "Epoch: 097, Train Loss: 0.1878, Test Reward: -0.1745\n",
      "Epoch: 098, Train Loss: 0.1849, Test Reward: -0.1719\n",
      "Epoch: 099, Train Loss: 0.1821, Test Reward: -0.1692\n",
      "Epoch: 100, Train Loss: 0.1793, Test Reward: -0.1667\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.9)\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    loss = train(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    print(f'Epoch: {epoch:03d}, Train Loss: {loss:.4f}, Test Reward: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroDataBatch(\n",
      "  \u001b[1muser\u001b[0m={\n",
      "    x=[5, 3],\n",
      "    batch=[5],\n",
      "    ptr=[2]\n",
      "  },\n",
      "  \u001b[1map\u001b[0m={\n",
      "    x=[3, 3],\n",
      "    batch=[3],\n",
      "    ptr=[2]\n",
      "  },\n",
      "  \u001b[1m(user, uplink, ap)\u001b[0m={\n",
      "    edge_attr=[15, 1],\n",
      "    edge_index=[2, 15]\n",
      "  },\n",
      "  \u001b[1m(ap, downlink, user)\u001b[0m={\n",
      "    edge_attr=[15, 1],\n",
      "    edge_index=[2, 15]\n",
      "  }\n",
      ")\n",
      "tensor([[ 0.1879, -0.1242,  0.0000],\n",
      "        [ 0.1879, -0.1242,  0.0000],\n",
      "        [ 0.1879, -0.1242,  0.0000],\n",
      "        [ 0.1879, -0.1242,  0.0000],\n",
      "        [ 0.1879, -0.1242,  0.0000]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = HetNetGNN(data, hidden_channels=64, out_channels=4, num_heads=2, num_layers=1)\n",
    "model = model.to(device)\n",
    "\n",
    "model.train()\n",
    "device_type = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "total_examples = total_loss = 0\n",
    "for batch in train_loader:\n",
    "    optimizer.zero_grad()\n",
    "    batch = batch.to(device_type)\n",
    "    break\n",
    "print(batch)\n",
    "\n",
    "output = model(batch.x_dict, batch.edge_index_dict)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroDataBatch(\n",
       "  \u001b[1muser\u001b[0m={\n",
       "    x=[5, 3],\n",
       "    batch=[5],\n",
       "    ptr=[2]\n",
       "  },\n",
       "  \u001b[1map\u001b[0m={\n",
       "    x=[3, 3],\n",
       "    batch=[3],\n",
       "    ptr=[2]\n",
       "  },\n",
       "  \u001b[1m(user, uplink, ap)\u001b[0m={\n",
       "    edge_attr=[15, 1],\n",
       "    edge_index=[2, 15]\n",
       "  },\n",
       "  \u001b[1m(ap, downlink, user)\u001b[0m={\n",
       "    edge_attr=[15, 1],\n",
       "    edge_index=[2, 15]\n",
       "  }\n",
       ")"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('user',\n",
       "  'uplink',\n",
       "  'ap'): tensor([[0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4],\n",
       "         [0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2]]),\n",
       " ('ap',\n",
       "  'downlink',\n",
       "  'user'): tensor([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2],\n",
       "         [0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4]])}"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.edge_index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('user', 'uplink', 'ap')\n",
      "user ap\n",
      "user__uplink__ap\n",
      "('ap', 'downlink', 'user')\n",
      "ap user\n",
      "ap__downlink__user\n"
     ]
    }
   ],
   "source": [
    "for edge_type, edge_index in batch.edge_index_dict.items():\n",
    "    x = edge_type\n",
    "    src_type, _, dst_type = edge_type\n",
    "    print(edge_type)\n",
    "    edge_type = '__'.join(edge_type)\n",
    "    print(src_type, dst_type)\n",
    "    print(edge_type)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ap', 'user')"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_type,dst_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (edge_type.split('__'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading http://snap.stanford.edu/ogb/data/nodeproppred/mag.zip\n"
     ]
    }
   ],
   "source": [
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import OGB_MAG\n",
    "from torch_geometric.nn import SAGEConv, to_hetero\n",
    "\n",
    "\n",
    "dataset = OGB_MAG(root='./data', preprocess='metapath2vec', transform=T.ToUndirected())\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
