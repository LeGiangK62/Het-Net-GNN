{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T02:02:52.595448600Z",
     "start_time": "2023-07-13T02:02:52.577445100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Giang\\Code\\GNN-Resource-Management\\NewDir\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get the current folder\n",
    "current_folder = os.getcwd()\n",
    "\n",
    "# Print the current folder\n",
    "print(current_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T02:02:53.287883600Z",
     "start_time": "2023-07-13T02:02:53.279882200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set the desired directory path\n",
    "new_directory = 'D:/Giang/Code/GNN-Resource-Management/NewDir'\n",
    "\n",
    "# Change the current directory\n",
    "os.chdir(new_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T02:02:53.828475500Z",
     "start_time": "2023-07-13T02:02:53.802469800Z"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "\n",
    "# Load .mat file\n",
    "data = scipy.io.loadmat('Allocation_8Ids_3APs.mat')\n",
    "\n",
    "# Access the variables\n",
    "Mu = data['Mu']\n",
    "Tau = data['Tau']\n",
    "Power = data['Power']\n",
    "ChannelAll = data['ChannelAll']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T02:00:14.900182Z",
     "start_time": "2023-07-13T02:00:14.891180500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1]], dtype=uint8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.19693747e+00, 1.48189778e+01, 5.76762579e+00, 6.77059628e-06,\n",
       "        6.19523472e+00, 1.08581442e-08, 1.32132602e+01, 4.76384006e+00]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.44011078e-07, 2.64449106e-05, 2.98598317e-06],\n",
       "       [2.15862365e-07, 6.78765907e-05, 1.19251445e-06],\n",
       "       [8.11006265e-07, 5.95050178e-06, 7.64526091e-06],\n",
       "       [2.56285471e-07, 8.60366171e-07, 1.31314600e-06],\n",
       "       [1.26782153e-06, 4.70406099e-07, 6.19311804e-07],\n",
       "       [1.82246142e-06, 2.20294781e-07, 2.47949891e-06],\n",
       "       [4.20545451e-06, 2.24559726e-07, 1.26098801e-06],\n",
       "       [4.05121889e-06, 9.69237742e-07, 7.51929511e-07]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ChannelAll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test WSN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import distance_matrix\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch.nn import Sequential as Seq, Linear as Lin, ReLU, Sigmoid, BatchNorm1d as BN\n",
    "\n",
    "from reImplement import GCNet\n",
    "from setup_arguments import setup_args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_channels_wsn(num_ap, num_user, num_samples, var_noise=1.0, radius=1):\n",
    "    # print(\"Generating Data for training and testing\")\n",
    "\n",
    "    # if num_ap != 1:\n",
    "    #     raise Exception(\"Can not generate data for training and testing with more than 1 base station\")\n",
    "    # generate position\n",
    "    dist_mat = []\n",
    "    position = []\n",
    "    index_user = np.tile(np.arange(num_user), (num_ap, 1))\n",
    "    index_ap = np.tile(np.arange(num_ap).reshape(-1, 1), (1, num_user))\n",
    "\n",
    "    index = np.array([index_user, index_ap])\n",
    "\n",
    "    # Calculate channel\n",
    "    CH = 1 / np.sqrt(2) * (np.random.randn(num_samples, 1, num_user)\n",
    "                           + 1j * np.random.randn(num_samples, 1, num_user))\n",
    "\n",
    "    if radius == 0:\n",
    "        Hs = abs(CH)\n",
    "    else:\n",
    "        for each_sample in range(num_samples):\n",
    "            pos = []\n",
    "            pos_BS = []\n",
    "\n",
    "            for i in range(num_ap):\n",
    "                r = radius * (np.random.rand())\n",
    "                theta = np.random.rand() * 2 * np.pi\n",
    "                pos_BS.append([r * np.sin(theta), r * np.cos(theta)])\n",
    "                pos.append([r * np.sin(theta), r * np.cos(theta)])\n",
    "            pos_user = []\n",
    "\n",
    "            for i in range(num_user):\n",
    "                r = 0.5 * radius + 0.5 * radius * np.random.rand()\n",
    "                theta = np.random.rand() * 2 * np.pi\n",
    "                pos_user.append([r * np.sin(theta), r * np.cos(theta)])\n",
    "                pos.append([r * np.sin(theta), r * np.cos(theta)])\n",
    "\n",
    "            pos = np.array(pos)\n",
    "            pos_BS = np.array(pos_BS)\n",
    "            dist_matrix = distance_matrix(pos_BS, pos_user)\n",
    "            # dist_matrixp = distance_matrix(pos[1:], pos[1:])\n",
    "            dist_mat.append(dist_matrix)\n",
    "            position.append(pos)\n",
    "\n",
    "        dist_mat = np.array(dist_mat)\n",
    "        position = np.array(position)\n",
    "\n",
    "        # Calculate Free space pathloss\n",
    "        # f = 2e9\n",
    "        # c = 3e8\n",
    "        # FSPL_old = 1 / ((4 * np.pi * f * dist_mat / c) ** 2)\n",
    "        FSPL = - (120.9 + 37.6 * np.log10(dist_mat/1000))\n",
    "        FSPL = 10 ** (FSPL / 10)\n",
    "\n",
    "        # print(f'FSPL_old:{FSPL_old.sum()}')\n",
    "        # print(f'FSPL_new:{FSPL.sum()}')\n",
    "        Hs = abs(CH * FSPL)\n",
    "\n",
    "    adj = adj_matrix(num_user * num_ap)\n",
    "\n",
    "    Hs, noise = normalize_matrix(Hs, var_noise)\n",
    "\n",
    "    return Hs, noise, position, adj, index\n",
    "\n",
    "def adj_matrix(num_nodes):\n",
    "    adj = []\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(num_nodes):\n",
    "            if not (i == j):\n",
    "                adj.append([i, j])\n",
    "    return np.array(adj)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_build(channel_matrix, index_matrix):\n",
    "    num_user, num_ap = channel_matrix.shape\n",
    "    adjacency_matrix = adj_matrix(num_user * num_ap)\n",
    "\n",
    "    index_user = np.reshape(index_matrix[0], (-1, 1))\n",
    "    index_ap = np.reshape(index_matrix[1], (-1, 1))\n",
    "\n",
    "    x1 = np.reshape(channel_matrix, (-1, 1))\n",
    "#     x2 = np.ones((num_user * num_ap, 1)) # power max here, for each?\n",
    "    x2 = np.zeros((num_user * num_ap, 1)) # ap selection\n",
    "    x3 = np.zeros((num_user * num_ap, 1))\n",
    "    x = np.concatenate((x1, x2, x3),axis=1)\n",
    "\n",
    "    edge_index = adjacency_matrix\n",
    "    edge_attr = []\n",
    "\n",
    "    for each_interference in adjacency_matrix:\n",
    "        tx = each_interference[0]\n",
    "        rx = each_interference[1]\n",
    "\n",
    "        tmp = [channel_matrix[index_ap[rx][0]][index_user[tx][0]]]\n",
    "#         tmp = [\n",
    "#             [channel_matrix[index_ap[rx][0]][index_user[tx][0]]],\n",
    "#             [channel_matrix[index_ap[tx][0]][index_user[rx][0]]]\n",
    "#         ]\n",
    "        edge_attr.append(tmp)\n",
    "\n",
    "    # y = np.expand_dims(channel_matrix, axis=0)\n",
    "    # pos = np.expand_dims(weights_matrix, axis=0)\n",
    "\n",
    "    data = Data(x=torch.tensor(x, dtype=torch.float),\n",
    "                edge_index=torch.tensor(edge_index, dtype=torch.long).t().contiguous(),\n",
    "                edge_attr=torch.tensor(edge_attr, dtype=torch.float),\n",
    "                # y=torch.tensor(y, dtype=torch.float),\n",
    "                # pos=torch.tensor(pos, dtype=torch.float)\n",
    "                )\n",
    "    return data\n",
    "\n",
    "def build_all_data(channel_matrices, index_mtx):\n",
    "    num_sample = channel_matrices.shape[0]\n",
    "    data_list = []\n",
    "    for i in range(num_sample):\n",
    "        data = graph_build(channel_matrices[i], index_mtx)\n",
    "        data_list.append(data)\n",
    "\n",
    "    return data_list\n",
    "\n",
    "def data_rate_calc(data, out, num_ap, num_user, noise_matrix, p_max, train = True, isLog=False):\n",
    "    G = torch.reshape(out[:, 0], (-1, num_ap, num_user))  #/ noise\n",
    "    ap_select = torch.reshape(out[:, 2], (-1, num_ap, num_user))\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # how to get channel from data and output\n",
    "    P = torch.reshape(out[:, 2], (-1, num_ap, num_user)) * p_max\n",
    "    P = torch.mul(P,ap_select)\n",
    "    desired_signal = torch.sum(torch.mul(P,G), dim=2).unsqueeze(-1)\n",
    "    P_UE = torch.sum(P, dim=1).unsqueeze(-1)\n",
    "    all_received_signal = torch.matmul(G, P_UE)\n",
    "    new_noise = torch.from_numpy(noise_matrix).to(device)\n",
    "    interference = all_received_signal - desired_signal + new_noise\n",
    "    rate = torch.log(1 + torch.div(desired_signal, interference))\n",
    "    sum_rate = torch.mean(torch.sum(rate, 1))\n",
    "    mean_power = torch.mean(torch.sum(P_UE, 1))\n",
    "\n",
    "    if(isLog):\n",
    "      print(f'Channel Coefficient: {G}')\n",
    "      print(f'Power: {P}')\n",
    "      print(f'desired_signal: {desired_signal}')\n",
    "      print(f'P_UE: {P_UE}')\n",
    "      print(f'all_received_signal: {all_received_signal}')\n",
    "      print(f'interference: {interference}')\n",
    "\n",
    "    if train:\n",
    "        return torch.neg(sum_rate/mean_power)\n",
    "    else:\n",
    "        return sum_rate/mean_power\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 3  # number of APs\n",
    "N = 5  # number of nodes\n",
    "R = 10  # radius\n",
    "\n",
    "num_train = 2  # number of training samples\n",
    "num_test = 4  # number of test samples\n",
    "\n",
    "reg = 1e-2\n",
    "pmax = 1\n",
    "var_db = 10\n",
    "var = 1 / 10 ** (var_db / 10)\n",
    "var_noise = 10e-11\n",
    "\n",
    "power_threshold = 2.0\n",
    "\n",
    "X_train, noise_train, pos_train, adj_train, index_train = generate_channels_wsn(K, N, num_train, var_noise, R)\n",
    "X_test, noise_test, pos_test, adj_test, index_test = generate_channels_wsn(K + 1, N + 10, num_test, var_noise, R)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing Data in to graph structured for model\n",
    "train_data_list = build_all_data(X_train, index_train)\n",
    "test_data_list = build_all_data(X_test, index_test)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = GCNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.9)\n",
    "\n",
    "train_loader = DataLoader(train_data_list, batch_size=10, shuffle=True, num_workers=1)\n",
    "test_loader = DataLoader(test_data_list, batch_size=10, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = []\n",
    "testing_loss = []\n",
    "# Training and Testing model\n",
    "for epoch in range(1, 100):\n",
    "    total_loss = 0\n",
    "    for each_data in train_loader:\n",
    "        data = each_data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = data_rate_calc(data, out, K, N, noise_train, power_threshold, train=True)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss = total_loss / num_train\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for each_data in test_loader:\n",
    "        data = each_data.to(device)\n",
    "        out = model(data)\n",
    "        loss = data_rate_calc(data, out, K + 1, N + 10, noise_test, power_threshold,  train=False)\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "\n",
    "    test_loss = total_loss / num_test\n",
    "\n",
    "    training_loss.append(train_loss)\n",
    "    testing_loss.append(test_loss)\n",
    "    if (epoch % 8 == 1):\n",
    "        print('Epoch {:03d}, Train Loss: {:.4f}, Val Loss: {:.4f}'.format(\n",
    "            epoch, train_loss, test_loss))\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edge Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "data = dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset.num_features: 1433\n",
      "dataset.num_classes : 7\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Planetoid' object has no attribute 'num_nodes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset.num_features: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;241m.\u001b[39mnum_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset.num_classes : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset.num_features: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;241m.\u001b[39mnum_nodes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset.num_classes : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\GNN-Resource\\lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:175\u001b[0m, in \u001b[0;36mInMemoryDataset.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    172\u001b[0m         data_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices()]\n\u001b[0;32m    173\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m Batch\u001b[38;5;241m.\u001b[39mfrom_data_list(data_list)[key]\n\u001b[1;32m--> 175\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    176\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Planetoid' object has no attribute 'num_nodes'"
     ]
    }
   ],
   "source": [
    "print(f'dataset.num_features: {dataset.num_features}')\n",
    "print(f'dataset.num_classes : {dataset.num_classes }')\n",
    "print(f'dataset.num_features: {dataset.num_nodes}')\n",
    "print(f'dataset.num_classes : {dataset.num_classes }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heterogenous Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import HeteroData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data = HeteroData()\n",
    "\n",
    "num_users = 5\n",
    "num_users_features = 3\n",
    "\n",
    "num_aps = 3\n",
    "num_aps_features = 3\n",
    "\n",
    "# Create two node types \"paper\" and \"author\" holding a feature matrix:\n",
    "data['user'].x = torch.randn(num_users, num_users_features) # features of user_node\n",
    "data['ap'].x = torch.randn(num_aps, num_aps_features) # features of user_node\n",
    "\n",
    "# Create edge types and building the\n",
    "# graph connectivity:\n",
    "data['user', 'up', 'ap'].edge_index = ...  # [2, num_edges]\n",
    "data['ap', 'down', 'user'].edge_index = ...  # [2, num_edges]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['user'].num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['ap'].num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['user', 'up', 'ap'].num_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['ap', 'user'].num_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  \u001b[1muser\u001b[0m={ x=[5, 3] },\n",
       "  \u001b[1map\u001b[0m={ x=[3, 3] },\n",
       "  \u001b[1mpaper\u001b[0m={},\n",
       "  \u001b[1m(user, up, ap)\u001b[0m={ edge_index=Ellipsis },\n",
       "  \u001b[1m(ap, down, user)\u001b[0m={ edge_index=Ellipsis }\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_dict = {\n",
    "    'ap': torch.tensor([0, 1]),\n",
    "    'user': torch.tensor([0, 2]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user': tensor([[ 1.6823,  0.8296,  1.7004],\n",
      "        [-0.5770, -1.0360, -1.2604],\n",
      "        [-0.0759,  1.3773, -0.9175],\n",
      "        [ 0.2248,  0.8405,  1.3123],\n",
      "        [ 0.9722, -0.1851,  0.0476]]), 'ap': tensor([[-0.9269,  1.0285,  0.5206],\n",
      "        [-1.2664, -0.6786, -0.1242],\n",
      "        [ 0.8528, -2.1334, -0.3032]])}\n"
     ]
    }
   ],
   "source": [
    "print(data.collect('x'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scipy  # for testing\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "from WSN_GNN import generate_channels_wsn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users = 5\n",
    "num_users_features = 3\n",
    "\n",
    "num_aps = 3\n",
    "num_aps_features = 3\n",
    "\n",
    "\n",
    "def convert_to_hetero_data(channel_matrices):\n",
    "    graph_list = []\n",
    "    num_sam, num_aps, num_users = channel_matrices.shape\n",
    "    for i in range(num_sam):\n",
    "        user_feat = torch.zeros(num_users, num_users_features)  # features of user_node\n",
    "        ap_feat = torch.zeros(num_aps, num_aps_features)  # features of user_node\n",
    "        edge_feat_uplink = channel_matrices[i, :, :].reshape(-1, 1)\n",
    "        edge_feat_downlink = channel_matrices[i, :, :].reshape(-1, 1)\n",
    "        graph = HeteroData({\n",
    "            'user': {'x': user_feat},\n",
    "            'ap': {'x': ap_feat}\n",
    "        })\n",
    "        # Create edge types and building the graph connectivity:\n",
    "        graph['user', 'uplink', 'ap'].edge_attr  = torch.tensor(edge_feat_uplink, dtype=torch.float)\n",
    "#         graph['ap', 'downlink', 'user'].edge_attr  = torch.tensor(edge_feat_downlink, dtype=torch.float)\n",
    "        graph_list.append(graph)\n",
    "    return graph_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'edge_attr': tensor([[0.0994],\n",
      "        [0.0209],\n",
      "        [0.1523],\n",
      "        [0.1551],\n",
      "        [0.3332],\n",
      "        [0.0233],\n",
      "        [0.0112],\n",
      "        [0.3298],\n",
      "        [1.0000],\n",
      "        [0.2695],\n",
      "        [0.3509],\n",
      "        [0.0279],\n",
      "        [0.0656],\n",
      "        [0.0477],\n",
      "        [0.0704]])}]\n"
     ]
    }
   ],
   "source": [
    "K = 3  # number of APs\n",
    "N = 5  # number of nodes\n",
    "R = 10  # radius\n",
    "\n",
    "num_train = 400  # number of training samples\n",
    "num_test = 4  # number of test samples\n",
    "\n",
    "reg = 1e-2\n",
    "pmax = 1\n",
    "var_db = 10\n",
    "var = 1 / 10 ** (var_db / 10)\n",
    "var_noise = 10e-11\n",
    "\n",
    "power_threshold = 2.0\n",
    "\n",
    "X_train, noise_train, pos_train, adj_train, index_train = generate_channels_wsn(K, N, num_train, var_noise, R)\n",
    "X_test, noise_test, pos_test, adj_test, index_test = generate_channels_wsn(K + 1, N + 10, num_test, var_noise, R)\n",
    "\n",
    "train_data = convert_to_hetero_data(X_train)\n",
    "print(train_data[0].edge_stores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  \u001b[1muser\u001b[0m={ x=[5, 3] },\n",
       "  \u001b[1map\u001b[0m={ x=[3, 3] },\n",
       "  \u001b[1m(user, uplink, ap)\u001b[0m={ edge_attr=[15, 1] }\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "batch_size = 100\n",
    "train_loader = DataLoader(train_data, batch_size, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://www.dropbox.com/s/yh4grpeks87ugr2/DBLP_processed.zip?dl=1\n",
      "Extracting D:\\Giang\\data\\DBLP\\raw\\DBLP_processed.zip\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  \u001b[1mauthor\u001b[0m={\n",
      "    x=[4057, 334],\n",
      "    y=[4057],\n",
      "    train_mask=[4057],\n",
      "    val_mask=[4057],\n",
      "    test_mask=[4057]\n",
      "  },\n",
      "  \u001b[1mpaper\u001b[0m={ x=[14328, 4231] },\n",
      "  \u001b[1mterm\u001b[0m={ x=[7723, 50] },\n",
      "  \u001b[1mconference\u001b[0m={\n",
      "    num_nodes=20,\n",
      "    x=[20, 1]\n",
      "  },\n",
      "  \u001b[1m(author, to, paper)\u001b[0m={ edge_index=[2, 19645] },\n",
      "  \u001b[1m(paper, to, author)\u001b[0m={ edge_index=[2, 19645] },\n",
      "  \u001b[1m(paper, to, term)\u001b[0m={ edge_index=[2, 85810] },\n",
      "  \u001b[1m(paper, to, conference)\u001b[0m={ edge_index=[2, 14328] },\n",
      "  \u001b[1m(term, to, paper)\u001b[0m={ edge_index=[2, 85810] },\n",
      "  \u001b[1m(conference, to, paper)\u001b[0m={ edge_index=[2, 14328] }\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import DBLP\n",
    "from torch_geometric.nn import HGTConv, Linear\n",
    "\n",
    "path = osp.join(osp.dirname(osp.realpath('.')), '../../data/DBLP')\n",
    "# We initialize conference node features with a single one-vector as feature:\n",
    "dataset = DBLP(path, transform=T.Constant(node_types='conference'))\n",
    "data = dataset[0]\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.4033, Train: 0.3325, Val: 0.2750, Test: 0.3245\n",
      "Epoch: 002, Loss: 1.3780, Train: 0.2950, Val: 0.2575, Test: 0.3012\n",
      "Epoch: 003, Loss: 1.3532, Train: 0.3300, Val: 0.2725, Test: 0.3107\n",
      "Epoch: 004, Loss: 1.3220, Train: 0.5775, Val: 0.4050, Test: 0.4691\n",
      "Epoch: 005, Loss: 1.2793, Train: 0.5900, Val: 0.4750, Test: 0.5207\n",
      "Epoch: 006, Loss: 1.2207, Train: 0.6025, Val: 0.4475, Test: 0.4980\n",
      "Epoch: 007, Loss: 1.1418, Train: 0.6575, Val: 0.4475, Test: 0.5041\n",
      "Epoch: 008, Loss: 1.0391, Train: 0.7450, Val: 0.4925, Test: 0.5453\n",
      "Epoch: 009, Loss: 0.9117, Train: 0.8100, Val: 0.5500, Test: 0.6122\n",
      "Epoch: 010, Loss: 0.7625, Train: 0.8550, Val: 0.6400, Test: 0.6712\n",
      "Epoch: 011, Loss: 0.6018, Train: 0.8625, Val: 0.6650, Test: 0.7148\n",
      "Epoch: 012, Loss: 0.4514, Train: 0.8625, Val: 0.6675, Test: 0.7228\n",
      "Epoch: 013, Loss: 0.3324, Train: 0.8650, Val: 0.6575, Test: 0.7292\n",
      "Epoch: 014, Loss: 0.2496, Train: 0.9100, Val: 0.6875, Test: 0.7482\n",
      "Epoch: 015, Loss: 0.1871, Train: 0.9850, Val: 0.7200, Test: 0.7725\n",
      "Epoch: 016, Loss: 0.1332, Train: 0.9975, Val: 0.7350, Test: 0.7915\n",
      "Epoch: 017, Loss: 0.0950, Train: 0.9975, Val: 0.7275, Test: 0.7811\n",
      "Epoch: 018, Loss: 0.0709, Train: 0.9975, Val: 0.7250, Test: 0.7771\n",
      "Epoch: 019, Loss: 0.0442, Train: 1.0000, Val: 0.7300, Test: 0.7872\n",
      "Epoch: 020, Loss: 0.0223, Train: 1.0000, Val: 0.7425, Test: 0.7894\n",
      "Epoch: 021, Loss: 0.0118, Train: 1.0000, Val: 0.7300, Test: 0.7885\n",
      "Epoch: 022, Loss: 0.0072, Train: 1.0000, Val: 0.7400, Test: 0.7921\n",
      "Epoch: 023, Loss: 0.0045, Train: 1.0000, Val: 0.7375, Test: 0.7891\n",
      "Epoch: 024, Loss: 0.0025, Train: 1.0000, Val: 0.7400, Test: 0.7878\n",
      "Epoch: 025, Loss: 0.0013, Train: 1.0000, Val: 0.7475, Test: 0.7863\n",
      "Epoch: 026, Loss: 0.0008, Train: 1.0000, Val: 0.7400, Test: 0.7832\n",
      "Epoch: 027, Loss: 0.0005, Train: 1.0000, Val: 0.7400, Test: 0.7829\n",
      "Epoch: 028, Loss: 0.0005, Train: 1.0000, Val: 0.7425, Test: 0.7820\n",
      "Epoch: 029, Loss: 0.0006, Train: 1.0000, Val: 0.7400, Test: 0.7811\n",
      "Epoch: 030, Loss: 0.0007, Train: 1.0000, Val: 0.7325, Test: 0.7802\n",
      "Epoch: 031, Loss: 0.0007, Train: 1.0000, Val: 0.7375, Test: 0.7796\n",
      "Epoch: 032, Loss: 0.0008, Train: 1.0000, Val: 0.7425, Test: 0.7768\n",
      "Epoch: 033, Loss: 0.0010, Train: 1.0000, Val: 0.7400, Test: 0.7756\n",
      "Epoch: 034, Loss: 0.0011, Train: 1.0000, Val: 0.7450, Test: 0.7765\n",
      "Epoch: 035, Loss: 0.0012, Train: 1.0000, Val: 0.7450, Test: 0.7796\n",
      "Epoch: 036, Loss: 0.0012, Train: 1.0000, Val: 0.7450, Test: 0.7814\n",
      "Epoch: 037, Loss: 0.0012, Train: 1.0000, Val: 0.7525, Test: 0.7811\n",
      "Epoch: 038, Loss: 0.0012, Train: 1.0000, Val: 0.7525, Test: 0.7783\n",
      "Epoch: 039, Loss: 0.0012, Train: 1.0000, Val: 0.7500, Test: 0.7786\n",
      "Epoch: 040, Loss: 0.0012, Train: 1.0000, Val: 0.7475, Test: 0.7768\n",
      "Epoch: 041, Loss: 0.0012, Train: 1.0000, Val: 0.7475, Test: 0.7768\n",
      "Epoch: 042, Loss: 0.0013, Train: 1.0000, Val: 0.7425, Test: 0.7774\n",
      "Epoch: 043, Loss: 0.0014, Train: 1.0000, Val: 0.7475, Test: 0.7774\n",
      "Epoch: 044, Loss: 0.0015, Train: 1.0000, Val: 0.7525, Test: 0.7799\n",
      "Epoch: 045, Loss: 0.0015, Train: 1.0000, Val: 0.7550, Test: 0.7817\n",
      "Epoch: 046, Loss: 0.0016, Train: 1.0000, Val: 0.7525, Test: 0.7811\n",
      "Epoch: 047, Loss: 0.0016, Train: 1.0000, Val: 0.7525, Test: 0.7796\n",
      "Epoch: 048, Loss: 0.0017, Train: 1.0000, Val: 0.7500, Test: 0.7796\n",
      "Epoch: 049, Loss: 0.0017, Train: 1.0000, Val: 0.7525, Test: 0.7805\n",
      "Epoch: 050, Loss: 0.0018, Train: 1.0000, Val: 0.7575, Test: 0.7826\n",
      "Epoch: 051, Loss: 0.0019, Train: 1.0000, Val: 0.7550, Test: 0.7829\n",
      "Epoch: 052, Loss: 0.0020, Train: 1.0000, Val: 0.7550, Test: 0.7839\n",
      "Epoch: 053, Loss: 0.0021, Train: 1.0000, Val: 0.7550, Test: 0.7817\n",
      "Epoch: 054, Loss: 0.0022, Train: 1.0000, Val: 0.7550, Test: 0.7808\n",
      "Epoch: 055, Loss: 0.0023, Train: 1.0000, Val: 0.7550, Test: 0.7811\n",
      "Epoch: 056, Loss: 0.0024, Train: 1.0000, Val: 0.7575, Test: 0.7820\n",
      "Epoch: 057, Loss: 0.0025, Train: 1.0000, Val: 0.7600, Test: 0.7829\n",
      "Epoch: 058, Loss: 0.0025, Train: 1.0000, Val: 0.7600, Test: 0.7848\n",
      "Epoch: 059, Loss: 0.0025, Train: 1.0000, Val: 0.7600, Test: 0.7845\n",
      "Epoch: 060, Loss: 0.0026, Train: 1.0000, Val: 0.7600, Test: 0.7835\n",
      "Epoch: 061, Loss: 0.0026, Train: 1.0000, Val: 0.7600, Test: 0.7829\n",
      "Epoch: 062, Loss: 0.0027, Train: 1.0000, Val: 0.7575, Test: 0.7835\n",
      "Epoch: 063, Loss: 0.0027, Train: 1.0000, Val: 0.7550, Test: 0.7839\n",
      "Epoch: 064, Loss: 0.0027, Train: 1.0000, Val: 0.7550, Test: 0.7823\n",
      "Epoch: 065, Loss: 0.0027, Train: 1.0000, Val: 0.7500, Test: 0.7814\n",
      "Epoch: 066, Loss: 0.0028, Train: 1.0000, Val: 0.7475, Test: 0.7814\n",
      "Epoch: 067, Loss: 0.0028, Train: 1.0000, Val: 0.7500, Test: 0.7808\n",
      "Epoch: 068, Loss: 0.0029, Train: 1.0000, Val: 0.7450, Test: 0.7814\n",
      "Epoch: 069, Loss: 0.0029, Train: 1.0000, Val: 0.7475, Test: 0.7820\n",
      "Epoch: 070, Loss: 0.0030, Train: 1.0000, Val: 0.7525, Test: 0.7820\n",
      "Epoch: 071, Loss: 0.0030, Train: 1.0000, Val: 0.7525, Test: 0.7820\n",
      "Epoch: 072, Loss: 0.0031, Train: 1.0000, Val: 0.7525, Test: 0.7817\n",
      "Epoch: 073, Loss: 0.0031, Train: 1.0000, Val: 0.7500, Test: 0.7811\n",
      "Epoch: 074, Loss: 0.0032, Train: 1.0000, Val: 0.7500, Test: 0.7808\n",
      "Epoch: 075, Loss: 0.0032, Train: 1.0000, Val: 0.7500, Test: 0.7817\n",
      "Epoch: 076, Loss: 0.0032, Train: 1.0000, Val: 0.7525, Test: 0.7820\n",
      "Epoch: 077, Loss: 0.0032, Train: 1.0000, Val: 0.7550, Test: 0.7832\n",
      "Epoch: 078, Loss: 0.0033, Train: 1.0000, Val: 0.7575, Test: 0.7839\n",
      "Epoch: 079, Loss: 0.0033, Train: 1.0000, Val: 0.7550, Test: 0.7848\n",
      "Epoch: 080, Loss: 0.0033, Train: 1.0000, Val: 0.7575, Test: 0.7845\n",
      "Epoch: 081, Loss: 0.0033, Train: 1.0000, Val: 0.7650, Test: 0.7848\n",
      "Epoch: 082, Loss: 0.0033, Train: 1.0000, Val: 0.7650, Test: 0.7845\n",
      "Epoch: 083, Loss: 0.0033, Train: 1.0000, Val: 0.7650, Test: 0.7854\n",
      "Epoch: 084, Loss: 0.0033, Train: 1.0000, Val: 0.7625, Test: 0.7854\n",
      "Epoch: 085, Loss: 0.0033, Train: 1.0000, Val: 0.7625, Test: 0.7851\n",
      "Epoch: 086, Loss: 0.0033, Train: 1.0000, Val: 0.7625, Test: 0.7848\n",
      "Epoch: 087, Loss: 0.0033, Train: 1.0000, Val: 0.7625, Test: 0.7845\n",
      "Epoch: 088, Loss: 0.0033, Train: 1.0000, Val: 0.7625, Test: 0.7848\n",
      "Epoch: 089, Loss: 0.0033, Train: 1.0000, Val: 0.7625, Test: 0.7851\n",
      "Epoch: 090, Loss: 0.0033, Train: 1.0000, Val: 0.7600, Test: 0.7845\n",
      "Epoch: 091, Loss: 0.0033, Train: 1.0000, Val: 0.7600, Test: 0.7848\n",
      "Epoch: 092, Loss: 0.0033, Train: 1.0000, Val: 0.7625, Test: 0.7854\n",
      "Epoch: 093, Loss: 0.0033, Train: 1.0000, Val: 0.7625, Test: 0.7854\n",
      "Epoch: 094, Loss: 0.0033, Train: 1.0000, Val: 0.7625, Test: 0.7857\n",
      "Epoch: 095, Loss: 0.0033, Train: 1.0000, Val: 0.7650, Test: 0.7857\n",
      "Epoch: 096, Loss: 0.0033, Train: 1.0000, Val: 0.7650, Test: 0.7860\n",
      "Epoch: 097, Loss: 0.0033, Train: 1.0000, Val: 0.7650, Test: 0.7866\n",
      "Epoch: 098, Loss: 0.0033, Train: 1.0000, Val: 0.7675, Test: 0.7869\n",
      "Epoch: 099, Loss: 0.0033, Train: 1.0000, Val: 0.7675, Test: 0.7869\n",
      "Epoch: 100, Loss: 0.0033, Train: 1.0000, Val: 0.7675, Test: 0.7875\n"
     ]
    }
   ],
   "source": [
    "class HGT(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, num_heads, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lin_dict = torch.nn.ModuleDict()\n",
    "        for node_type in data.node_types:\n",
    "            self.lin_dict[node_type] = Linear(-1, hidden_channels)\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            conv = HGTConv(hidden_channels, hidden_channels, data.metadata(),\n",
    "                           num_heads, group='sum')\n",
    "            self.convs.append(conv)\n",
    "\n",
    "        self.lin = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        x_dict = {\n",
    "            node_type: self.lin_dict[node_type](x).relu_()\n",
    "            for node_type, x in x_dict.items()\n",
    "        }\n",
    "\n",
    "        for conv in self.convs:\n",
    "            x_dict = conv(x_dict, edge_index_dict)\n",
    "\n",
    "        return self.lin(x_dict['author'])\n",
    "\n",
    "\n",
    "model = HGT(hidden_channels=64, out_channels=4, num_heads=2, num_layers=1)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data, model = data.to(device), model.to(device)\n",
    "\n",
    "with torch.no_grad():  # Initialize lazy modules.\n",
    "    out = model(data.x_dict, data.edge_index_dict)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=0.001)\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x_dict, data.edge_index_dict)\n",
    "    mask = data['author'].train_mask\n",
    "    loss = F.cross_entropy(out[mask], data['author'].y[mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    pred = model(data.x_dict, data.edge_index_dict).argmax(dim=-1)\n",
    "\n",
    "    accs = []\n",
    "    for split in ['train_mask', 'val_mask', 'test_mask']:\n",
    "        mask = data['author'][split]\n",
    "        acc = (pred[mask] == data['author'].y[mask]).sum() / mask.sum()\n",
    "        accs.append(float(acc))\n",
    "    return accs\n",
    "\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    train_acc, val_acc, test_acc = test()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train: {train_acc:.4f}, '\n",
    "          f'Val: {val_acc:.4f}, Test: {test_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239566\n"
     ]
    }
   ],
   "source": [
    "print(data.num_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj_matrix(num_aps, num_users):\n",
    "    adj = []\n",
    "    for i in range(num_aps):\n",
    "        for j in range(num_users):\n",
    "            adj.append([i, j])\n",
    "    return np.array(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = adj_matrix(3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2],\n",
       "       [0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tesst Data Hetero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance_matrix\n",
    "def generate_channels_wsn(num_ap, num_user, num_samples, var_noise=1.0, radius=1):\n",
    "    # print(\"Generating Data for training and testing\")\n",
    "\n",
    "    # if num_ap != 1:\n",
    "    #     raise Exception(\"Can not generate data for training and testing with more than 1 base station\")\n",
    "    # generate position\n",
    "    dist_mat = []\n",
    "    position = []\n",
    "    index_user = np.tile(np.arange(num_user), (num_ap, 1))\n",
    "    index_ap = np.tile(np.arange(num_ap).reshape(-1, 1), (1, num_user))\n",
    "\n",
    "    index = np.array([index_user, index_ap])\n",
    "\n",
    "    # Calculate channel\n",
    "    CH = 1 / np.sqrt(2) * (np.random.randn(num_samples, 1, num_user)\n",
    "                           + 1j * np.random.randn(num_samples, 1, num_user))\n",
    "\n",
    "    if radius == 0:\n",
    "        Hs = abs(CH)\n",
    "    else:\n",
    "        for each_sample in range(num_samples):\n",
    "            pos = []\n",
    "            pos_BS = []\n",
    "\n",
    "            for i in range(num_ap):\n",
    "                r = radius * (np.random.rand())\n",
    "                theta = np.random.rand() * 2 * np.pi\n",
    "                pos_BS.append([r * np.sin(theta), r * np.cos(theta)])\n",
    "                pos.append([r * np.sin(theta), r * np.cos(theta)])\n",
    "            pos_user = []\n",
    "\n",
    "            for i in range(num_user):\n",
    "                r = 0.5 * radius + 0.5 * radius * np.random.rand()\n",
    "                theta = np.random.rand() * 2 * np.pi\n",
    "                pos_user.append([r * np.sin(theta), r * np.cos(theta)])\n",
    "                pos.append([r * np.sin(theta), r * np.cos(theta)])\n",
    "\n",
    "            pos = np.array(pos)\n",
    "            pos_BS = np.array(pos_BS)\n",
    "            dist_matrix = distance_matrix(pos_BS, pos_user)\n",
    "            # dist_matrixp = distance_matrix(pos[1:], pos[1:])\n",
    "            dist_mat.append(dist_matrix)\n",
    "            position.append(pos)\n",
    "\n",
    "        dist_mat = np.array(dist_mat)\n",
    "        position = np.array(position)\n",
    "\n",
    "        # Calculate Free space pathloss\n",
    "        # f = 2e9\n",
    "        # c = 3e8\n",
    "        # FSPL_old = 1 / ((4 * np.pi * f * dist_mat / c) ** 2)\n",
    "        FSPL = - (120.9 + 37.6 * np.log10(dist_mat/1000))\n",
    "        FSPL = 10 ** (FSPL / 10)\n",
    "\n",
    "        # print(f'FSPL_old:{FSPL_old.sum()}')\n",
    "        # print(f'FSPL_new:{FSPL.sum()}')\n",
    "        Hs = abs(CH * FSPL)\n",
    "\n",
    "    Hs, noise = normalize_matrix(Hs, var_noise)\n",
    "\n",
    "    return Hs, noise, position, index\n",
    "\n",
    "def channel_reshape(channel, num_ap, num_user):\n",
    "    # input of (num_samples x 1)\n",
    "    # output of (num_samples x num_ap x num_user)\n",
    "    tmp = np.repeat(\n",
    "        np.expand_dims(\n",
    "            np.repeat(\n",
    "                channel,\n",
    "                num_ap, axis=1),\n",
    "            axis=2\n",
    "        ),\n",
    "        num_user,\n",
    "        axis=2\n",
    "    )\n",
    "    return tmp\n",
    "\n",
    "\n",
    "def normalize_matrix(channel_matrix, noise_var):\n",
    "    num_samples, num_ap, num_user = channel_matrix.shape\n",
    "    max_each_sample = np.expand_dims(\n",
    "        np.max(\n",
    "            np.max(channel_matrix, axis=2),\n",
    "            axis=-1\n",
    "        ),\n",
    "        axis=1\n",
    "        )\n",
    "    max_matrix = channel_reshape(max_each_sample, num_ap, num_user)\n",
    "\n",
    "    min_each_sample = np.expand_dims(\n",
    "        np.min(\n",
    "            np.min(channel_matrix, axis=2),\n",
    "            axis=-1\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    min_matrix = channel_reshape(min_each_sample, num_ap, num_user)\n",
    "\n",
    "    noise = np.ones(channel_matrix.shape) * noise_var\n",
    "\n",
    "    # return (\n",
    "    #     (channel_matrix - min_matrix) / (max_matrix - min_matrix),\n",
    "    #     (noise - min_matrix) / (max_matrix - min_matrix),\n",
    "    # )\n",
    "    return (\n",
    "        (channel_matrix / max_matrix),\n",
    "        (noise / max_matrix),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_hetero_data(channel_matrices):\n",
    "    graph_list = []\n",
    "    num_sam, num_aps, num_users = channel_matrices.shape\n",
    "    for i in range(num_sam):\n",
    "        user_feat = torch.randn(num_users, num_users_features)  # features of user_node\n",
    "        ap_feat = torch.randn(num_aps, num_aps_features)  # features of user_node\n",
    "        edge_feat_uplink = channel_matrices[i, :, :].reshape(-1, 1)\n",
    "        edge_feat_downlink = channel_matrices[i, :, :].reshape(-1, 1)\n",
    "        graph = HeteroData({\n",
    "            'user': {'x': user_feat},\n",
    "            'ap': {'x': ap_feat}\n",
    "        })\n",
    "        # Create edge types and building the graph connectivity:\n",
    "        graph['user', 'uplink', 'ap'].edge_attr = torch.tensor(edge_feat_uplink, dtype=torch.float)\n",
    "        graph['user', 'uplink', 'ap'].edge_index = torch.tensor(adj_matrix(num_users, num_aps).transpose(), dtype=torch.int64)\n",
    "        graph['ap', 'downlink', 'user'].edge_index = torch.tensor(adj_matrix(num_aps, num_users).transpose(),\n",
    "                                                                dtype=torch.int64)\n",
    "\n",
    "        # graph['ap', 'downlink', 'user'].edge_attr  = torch.tensor(edge_feat_downlink, dtype=torch.float)\n",
    "        graph_list.append(graph)\n",
    "    return graph_list\n",
    "\n",
    "def adj_matrix(num_from, num_dest):\n",
    "    adj = []\n",
    "    for i in range(num_from):\n",
    "        for j in range(num_dest):\n",
    "            adj.append([i, j])\n",
    "    return np.array(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 3  # number of APs\n",
    "N = 5  # number of nodes\n",
    "R = 10  # radius\n",
    "\n",
    "num_users_features = 3\n",
    "num_aps_features = 3\n",
    "\n",
    "num_train = 2  # number of training samples\n",
    "num_test = 4  # number of test samples\n",
    "\n",
    "reg = 1e-2\n",
    "pmax = 1\n",
    "var_db = 10\n",
    "var = 1 / 10 ** (var_db / 10)\n",
    "var_noise = 10e-11\n",
    "\n",
    "power_threshold = 2.0\n",
    "\n",
    "X_train, noise_train, pos_train, index_train = generate_channels_wsn(K, N, num_train, var_noise, R)\n",
    "X_test, noise_test, pos_test, index_test = generate_channels_wsn(K + 1, N + 10, num_test, var_noise, R)\n",
    "\n",
    "# Maybe need normalization here\n",
    "train_data = convert_to_hetero_data(X_train)\n",
    "test_data = convert_to_hetero_data(X_test)\n",
    "\n",
    "batchSize = 100\n",
    "train_loader = DataLoader(train_data, batchSize, shuffle=True, num_workers=1)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "data = train_data[0]\n",
    "data = data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0613, -0.0862,  0.1738, -0.0829],\n",
       "        [-0.0640, -0.0097,  0.0460, -0.0183],\n",
       "        [ 0.0135, -0.1324,  0.1521, -0.1092],\n",
       "        [-0.0988, -0.1225,  0.1574, -0.0855],\n",
       "        [-0.0390, -0.0085,  0.0239, -0.0096]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data['user', 'uplink', 'ap']['edge_attr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#region Build Heterogeneous GNN\n",
    "class HetNetGNN(torch.nn.Module):\n",
    "    def __init__(self, data, hidden_channels, out_channels, num_heads, num_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lin_dict = torch.nn.ModuleDict()\n",
    "        for node_type in data.node_types:\n",
    "            self.lin_dict[node_type] = Linear(-1, hidden_channels)\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            conv = HGTConv(hidden_channels, hidden_channels, data.metadata(),\n",
    "                           num_heads, group='sum')\n",
    "            self.convs.append(conv)\n",
    "\n",
    "        self.lin = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        x_dict = {\n",
    "            node_type: self.lin_dict[node_type](x).relu_()\n",
    "            for node_type, x in x_dict.items()\n",
    "        }\n",
    "\n",
    "        for conv in self.convs:\n",
    "            x_dict = conv(x_dict, edge_index_dict)\n",
    "\n",
    "        return self.lin(x_dict['user'])\n",
    "\n",
    "\n",
    "#endregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HetNetGNN(data, hidden_channels=64, out_channels=4, num_heads=2, num_layers=1)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1480, -0.1011, -0.1282, -0.0549],\n",
      "        [-0.1823, -0.1279, -0.1350, -0.0248],\n",
      "        [-0.1425, -0.1360, -0.0933, -0.0341],\n",
      "        [-0.1568, -0.0868, -0.0459,  0.0029],\n",
      "        [-0.1772, -0.0984, -0.1865, -0.0762]])\n",
      "HeteroData(\n",
      "  \u001b[1muser\u001b[0m={ x=[5, 3] },\n",
      "  \u001b[1map\u001b[0m={ x=[3, 3] },\n",
      "  \u001b[1m(user, uplink, ap)\u001b[0m={\n",
      "    edge_attr=[15, 1],\n",
      "    edge_index=[2, 15]\n",
      "  },\n",
      "  \u001b[1m(ap, downlink, user)\u001b[0m={ edge_index=[2, 15] }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(data.x_dict, data.edge_index_dict)\n",
    "print(output)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "input batch data and output of the model\n",
    "output: the enegy efficieny or the sumRate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(data, out, num_ap, num_user, noise_matrix, p_max, train=True, is_log=False):\n",
    "    # Loss function only takes data and the output to calculate energy efficiency\n",
    "\n",
    "    G = torch.reshape(out[:, 0], (-1, num_ap, num_user))  #/ noise\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # how to get channel from data and output\n",
    "    P = torch.reshape(out[:, 2], (-1, num_ap, num_user)) * p_max\n",
    "    # ## ap selection part\n",
    "    # ap_select = torch.reshape(out[:, 1], (-1, num_ap, num_user))\n",
    "    # P = torch.mul(P, ap_select)\n",
    "    # ##\n",
    "    desired_signal = torch.sum(torch.mul(P,G), dim=2).unsqueeze(-1)\n",
    "    P_UE = torch.sum(P, dim=1).unsqueeze(-1)\n",
    "    all_received_signal = torch.matmul(G, P_UE)\n",
    "    new_noise = torch.from_numpy(noise_matrix).to(device)\n",
    "    interference = all_received_signal - desired_signal + new_noise\n",
    "    rate = torch.log(1 + torch.div(desired_signal, interference))\n",
    "    sum_rate = torch.mean(torch.sum(rate, 1))\n",
    "    mean_power = torch.mean(torch.sum(P_UE, 1))\n",
    "\n",
    "    if is_log:\n",
    "        print(f'Channel Coefficient: {G}')\n",
    "        print(f'Power: {P}')\n",
    "        print(f'desired_signal: {desired_signal}')\n",
    "        print(f'P_UE: {P_UE}')\n",
    "        print(f'all_received_signal: {all_received_signal}')\n",
    "        print(f'interference: {interference}')\n",
    "\n",
    "    if train:\n",
    "        return torch.neg(sum_rate/mean_power)\n",
    "    else:\n",
    "        return sum_rate/mean_power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(output, batch):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    ##\n",
    "    channel_matrix = batch['user', 'ap']['edge_attr']\n",
    "    power = batch['user']['x'][:,0]\n",
    "    G = torch.reshape(channel_matrix, (-1, num_ap, num_user))\n",
    "    P = torch.reshape(power, (-1, num_ap, num_user)) * p_max\n",
    "    \n",
    "    ##\n",
    "    desired_signal = torch.sum(torch.mul(P,G), dim=2).unsqueeze(-1)\n",
    "    P_UE = torch.sum(P, dim=1).unsqueeze(-1)\n",
    "    all_received_signal = torch.matmul(G, P_UE)\n",
    "    new_noise = torch.from_numpy(noise_matrix).to(device)\n",
    "    interference = all_received_signal - desired_signal + new_noise\n",
    "    rate = torch.log(1 + torch.div(desired_signal, interference))\n",
    "    sum_rate = torch.mean(torch.sum(rate, 1))\n",
    "    mean_power = torch.mean(torch.sum(P_UE, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['user']['x'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000],\n",
       "        [0.0117],\n",
       "        [0.0073],\n",
       "        [0.0129],\n",
       "        [0.0442],\n",
       "        [0.0207],\n",
       "        [0.0050],\n",
       "        [0.1102],\n",
       "        [0.0061],\n",
       "        [0.0140],\n",
       "        [0.0025],\n",
       "        [0.0023],\n",
       "        [0.3133],\n",
       "        [0.0029]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['user', 'ap']['edge_attr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroDataBatch(\n",
       "  \u001b[1muser\u001b[0m={\n",
       "    x=[10, 3],\n",
       "    batch=[10],\n",
       "    ptr=[3]\n",
       "  },\n",
       "  \u001b[1map\u001b[0m={\n",
       "    x=[6, 3],\n",
       "    batch=[6],\n",
       "    ptr=[3]\n",
       "  },\n",
       "  \u001b[1m(user, uplink, ap)\u001b[0m={\n",
       "    edge_attr=[28, 1],\n",
       "    edge_index=[2, 30]\n",
       "  },\n",
       "  \u001b[1m(ap, downlink, user)\u001b[0m={ edge_index=[2, 30] }\n",
       ")"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3])\n"
     ]
    }
   ],
   "source": [
    "power = batch['user']['x']\n",
    "print(power.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.7116e-04],\n",
      "        [6.8099e-04],\n",
      "        [2.9243e-03],\n",
      "        [8.0180e-04],\n",
      "        [1.0000e+00],\n",
      "        [3.9735e-03],\n",
      "        [1.0341e-03],\n",
      "        [1.4105e-01],\n",
      "        [2.5859e-03],\n",
      "        [5.5022e-03],\n",
      "        [2.5087e-04],\n",
      "        [6.4103e-03],\n",
      "        [9.3652e-05],\n",
      "        [1.3648e-03],\n",
      "        [1.0000e+00],\n",
      "        [1.1707e-02],\n",
      "        [7.3273e-03],\n",
      "        [1.2917e-02],\n",
      "        [4.4212e-02],\n",
      "        [2.0658e-02],\n",
      "        [4.9644e-03],\n",
      "        [1.1016e-01],\n",
      "        [6.1488e-03],\n",
      "        [1.3968e-02],\n",
      "        [2.5157e-03],\n",
      "        [2.3086e-03],\n",
      "        [3.1334e-01],\n",
      "        [2.9395e-03]])\n"
     ]
    }
   ],
   "source": [
    "channel_matrix = batch['user', 'ap']['edge_attr']\n",
    "print(channel_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_user = 5\n",
    "x1 = torch.ones(num_user,1)\n",
    "x2 = torch.ones(num_user,1) # power allocation\n",
    "x3 = torch.ones(num_user,1) # ap selection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((x1,x2,x3),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_matrix = batch['user', 'ap']['edge_attr']\n",
    "\n",
    "power_max = batch['user']['x'][:, 0]\n",
    "power = batch['user']['x'][:, 1]\n",
    "ap_selection = batch['user']['x'][:, 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.2864, -0.4920,  0.9406,  1.1642,  0.4339, -0.6288, -0.2291,  0.8168,\n",
       "        -1.0453,  0.3287])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "power_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "power.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "ap_selection = torch.tensor([0, 1, 2, 3, 1, 2, 3, 2, 1, 0])\n",
    "print(ap_selection.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "P = torch.zeros(1, 4,10)\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(1)\n",
      "tensor(2)\n",
      "tensor(3)\n",
      "tensor(2)\n",
      "tensor(1)\n",
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    tmp = power_max[i] * power[i]\n",
    "    P[0][ap_selection[i]][i] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0857,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0533],\n",
       "         [ 0.0000,  0.0941,  0.0000,  0.0000,  0.1711,  0.0000,  0.0000,\n",
       "           0.0000,  0.0376,  0.0000],\n",
       "         [ 0.0000,  0.0000, -0.9847,  0.0000,  0.0000,  0.3065,  0.0000,\n",
       "          -0.3427,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.1886,  0.0000,  0.0000, -0.0096,\n",
       "           0.0000,  0.0000,  0.0000]]])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Assuming power_max, power, and P are torch tensors\n",
    "\n",
    "# Compute the element-wise product of power_max and power\n",
    "tmp = power_max * power\n",
    "\n",
    "# Create an index tensor for the first dimension of P\n",
    "index = torch.arange(10)\n",
    "\n",
    "# Update the corresponding elements in P using tensor indexing\n",
    "P[0, ap_selection[index], index] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0857,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "           0.0000,  0.0000,  0.0533],\n",
       "         [ 0.0000,  0.0941,  0.0000,  0.0000,  0.1711,  0.0000,  0.0000,\n",
       "           0.0000,  0.0376,  0.0000],\n",
       "         [ 0.0000,  0.0000, -0.9847,  0.0000,  0.0000,  0.3065,  0.0000,\n",
       "          -0.3427,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.1886,  0.0000,  0.0000, -0.0096,\n",
       "           0.0000,  0.0000,  0.0000]]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
